{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from dataloader import APPLIANCE_ORDER, get_train_test\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from unsupervised_aug import augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda_av = False\n",
    "if torch.cuda.is_available():\n",
    "    cuda_av = True\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "weight_appliance = {'mw': 1, 'dw': 1, 'dr': 1, 'fridge': 1, 'hvac': 1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, cell_type, hidden_size, num_layers, bidirectional):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        if cell_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(input_size=1, hidden_size=hidden_size,\n",
    "                              num_layers=num_layers, batch_first=True,\n",
    "                              bidirectional=bidirectional)\n",
    "        elif cell_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(input_size=1, hidden_size=hidden_size,\n",
    "                              num_layers=num_layers, batch_first=True,\n",
    "                              bidirectional=bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(input_size=1, hidden_size=hidden_size,\n",
    "                               num_layers=num_layers, batch_first=True,\n",
    "                               bidirectional=bidirectional)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size * self.num_directions, 1)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred, hidden = self.rnn(x, None)\n",
    "        pred = self.linear(pred).view(pred.data.shape[0], -1, 1)\n",
    "        # pred = self.act(pred)\n",
    "        # pred = torch.clamp(pred, min=0.)\n",
    "        # pred = self.act(pred)\n",
    "        pred = torch.min(pred, x)\n",
    "        return pred\n",
    "\n",
    "\n",
    "class AppliancesRNN(nn.Module):\n",
    "    def __init__(self, cell_type, hidden_size, num_layers, bidirectional, num_appliance):\n",
    "        super(AppliancesRNN, self).__init__()\n",
    "        self.num_appliance = num_appliance\n",
    "        self.preds = {}\n",
    "        self.order = ORDER\n",
    "        for appliance in range(self.num_appliance):\n",
    "            if cuda_av:\n",
    "                setattr(self, \"Appliance_\" + str(appliance), CustomRNN(cell_type,\n",
    "                                                                       hidden_size,\n",
    "                                                                       num_layers,\n",
    "                                                                       bidirectional).cuda())\n",
    "            else:\n",
    "                setattr(self, \"Appliance_\" + str(appliance), CustomRNN(cell_type,\n",
    "                                                                       hidden_size,\n",
    "                                                                       num_layers,\n",
    "                                                                       bidirectional))\n",
    "\n",
    "    def forward(self, *args):\n",
    "        agg_current = args[0]\n",
    "        flag = False\n",
    "        if np.random.random() > args[1]:\n",
    "            flag = True\n",
    "            # print(\"Subtracting prediction\")\n",
    "        else:\n",
    "            pass\n",
    "            # print(\"Subtracting true\")\n",
    "        for appliance in range(self.num_appliance):\n",
    "            # print(agg_current.mean().data[0])\n",
    "            # print (appliance)\n",
    "            # print (self.order[appliance])\n",
    "            # print (args[2+appliance])\n",
    "            # print(getattr(self, \"Appliance_\" + str(appliance)))\n",
    "            self.preds[appliance] = getattr(self, \"Appliance_\" + str(appliance))(agg_current)\n",
    "            if flag:\n",
    "                agg_current = agg_current - self.preds[appliance]\n",
    "            else:\n",
    "                agg_current = agg_current - args[2 + appliance]\n",
    "\n",
    "        return torch.cat([self.preds[a] for a in range(self.num_appliance)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disagg_fold(num_aug, case, fold_num, dataset, cell_type, hidden_size, num_layers, bidirectional, lr, num_iterations, p):\n",
    "    # print (fold_num, hidden_size, num_layers, bidirectional, lr, num_iterations, p)\n",
    "    print (ORDER)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    train, test = get_train_test(dataset, num_folds=num_folds, fold_num=fold_num)\n",
    "    print (train.shape[0])\n",
    "    train = augmented_data(train, num_aug, case, test, True)\n",
    "    print (train.shape[0])\n",
    "    train_aggregate = train[:, 0, :, :].reshape(-1, 24, 1)\n",
    "    test_aggregate = test[:, 0, :, :].reshape(-1, 24, 1)\n",
    "\n",
    "    out_train = [None for temp in range(len(ORDER))]\n",
    "    for a_num, appliance in enumerate(ORDER):\n",
    "        out_train[a_num] = Variable(\n",
    "            torch.Tensor(train[:, APPLIANCE_ORDER.index(appliance), :, :].reshape((train_aggregate.shape[0], -1, 1))))\n",
    "        if cuda_av:\n",
    "            out_train[a_num] = out_train[a_num].cuda()\n",
    "\n",
    "    loss_func = nn.L1Loss()\n",
    "    a = AppliancesRNN(cell_type, hidden_size, num_layers, bidirectional, len(ORDER))\n",
    "\n",
    "    if cuda_av:\n",
    "        a = a.cuda()\n",
    "        loss_func = loss_func.cuda()\n",
    "    optimizer = torch.optim.Adam(a.parameters(), lr=lr)\n",
    "\n",
    "    inp = Variable(torch.Tensor(train_aggregate.reshape((train_aggregate.shape[0], -1, 1))).type(torch.FloatTensor),\n",
    "                   requires_grad=True)\n",
    "    for t in range(num_iterations):\n",
    "        inp = Variable(torch.Tensor(train_aggregate), requires_grad=True)\n",
    "        out = torch.cat([out_train[appliance_num] for appliance_num, appliance in enumerate(ORDER)])\n",
    "        if cuda_av:\n",
    "            inp = inp.cuda()\n",
    "            out = out.cuda()\n",
    "\n",
    "        params = [inp, p]\n",
    "        for a_num, appliance in enumerate(ORDER):\n",
    "            params.append(out_train[a_num])\n",
    "        # print(params)\n",
    "        pred = a(*params)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_func(pred, out)\n",
    "        if t % 100 == 0:\n",
    "            print(t, loss.data[0])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test_inp = Variable(torch.Tensor(test_aggregate), requires_grad=False)\n",
    "    if cuda_av:\n",
    "        test_inp = test_inp.cuda()\n",
    "\n",
    "    params = [test_inp, -2]\n",
    "    for i in range(len(ORDER)):\n",
    "        params.append(None)\n",
    "    pr = a(*params)\n",
    "    pr = torch.clamp(pr, min=0.)\n",
    "    test_pred = torch.split(pr, test_aggregate.shape[0])\n",
    "    prediction_fold = [None for x in range(len(ORDER))]\n",
    "\n",
    "    if cuda_av:\n",
    "        for appliance_num, appliance in enumerate(ORDER):\n",
    "            prediction_fold[appliance_num] = test_pred[appliance_num].cpu().data.numpy().reshape(-1, 24)\n",
    "    else:\n",
    "        for appliance_num, appliance in enumerate(ORDER):\n",
    "            prediction_fold[appliance_num] = test_pred[appliance_num].data.numpy().reshape(-1, 24)\n",
    "    gt_fold = [None for x in range(len(ORDER))]\n",
    "    for appliance_num, appliance in enumerate(ORDER):\n",
    "        gt_fold[appliance_num] = test[:, APPLIANCE_ORDER.index(appliance), :, :].reshape(test_aggregate.shape[0], -1,\n",
    "                                                                                         1).reshape(-1, 24)\n",
    "\n",
    "    return prediction_fold, gt_fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_params = np.load(\"../code/baseline/result/rnn-tree-param-3.npy\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'bidirectional': True,\n",
       "  'cell_type': 'LSTM',\n",
       "  'hidden_size': 50,\n",
       "  'iters': 3000,\n",
       "  'lr': 0.01,\n",
       "  'num_layers': 2,\n",
       "  'order': ('fridge', 'hvac', 'dr', 'mw', 'dw')},\n",
       " 1: {'bidirectional': True,\n",
       "  'cell_type': 'GRU',\n",
       "  'hidden_size': 100,\n",
       "  'iters': 3000,\n",
       "  'lr': 0.01,\n",
       "  'num_layers': 1,\n",
       "  'order': ('fridge', 'dw', 'mw', 'dr', 'hvac')},\n",
       " 2: {'bidirectional': True,\n",
       "  'cell_type': 'LSTM',\n",
       "  'hidden_size': 50,\n",
       "  'iters': 3000,\n",
       "  'lr': 0.01,\n",
       "  'num_layers': 2,\n",
       "  'order': ('dw', 'dr', 'fridge', 'mw', 'hvac')},\n",
       " 3: {'bidirectional': True,\n",
       "  'cell_type': 'GRU',\n",
       "  'hidden_size': 100,\n",
       "  'iters': 3000,\n",
       "  'lr': 0.01,\n",
       "  'num_layers': 1,\n",
       "  'order': ('fridge', 'mw', 'hvac', 'dw', 'dr')},\n",
       " 4: {'bidirectional': True,\n",
       "  'cell_type': 'LSTM',\n",
       "  'hidden_size': 50,\n",
       "  'iters': 3000,\n",
       "  'lr': 0.01,\n",
       "  'num_layers': 2,\n",
       "  'order': ('fridge', 'dr', 'mw', 'dw', 'hvac')}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvac 0\n",
      "LSTM 100 2 True 0.01 2000\n",
      "['hvac']\n",
      "54\n",
      "54\n",
      "0 892.68359375\n",
      "100 742.0939331054688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bd0906608e11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcell_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mpred_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisagg_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mgts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8d9da0c36615>\u001b[0m in \u001b[0;36mdisagg_fold\u001b[0;34m(num_aug, case, fold_num, dataset, cell_type, hidden_size, num_layers, bidirectional, lr, num_iterations, p)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ORDER=['hvac']\n",
    "preds = {}\n",
    "gts = {}\n",
    "num_folds = 5\n",
    "dataset = 1\n",
    "p=0\n",
    "\n",
    "case=1\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "for num_aug in [0, 20, 50, 100]:\n",
    "    for appliance in ['hvac']:\n",
    "        ORDER = [appliance]\n",
    "        preds = []\n",
    "        gts = []\n",
    "        for fold_num in range(5):\n",
    "\n",
    "            print (appliance, fold_num)\n",
    "\n",
    "            cell_type = best_params[dataset][fold_num][appliance]['cell_type']\n",
    "            hidden_size = best_params[dataset][fold_num][appliance]['hidden_size']\n",
    "            num_layers = best_params[dataset][fold_num][appliance]['num_layers']\n",
    "            bidirectional = best_params[dataset][fold_num][appliance]['bidirectional']\n",
    "            lr = best_params[dataset][fold_num][appliance]['lr']\n",
    "            num_iterations = best_params[dataset][fold_num][appliance]['iters']\n",
    "\n",
    "            print (cell_type, hidden_size, num_layers, bidirectional, lr, num_iterations)\n",
    "            pred_fold, gt_fold = disagg_fold(num_aug, case, fold_num, dataset, cell_type, hidden_size, num_layers, bidirectional, lr, num_iterations, 0)\n",
    "            preds.append(pred_fold)\n",
    "            gts.append(gt_fold)\n",
    "\n",
    "        prediction_flatten = {}\n",
    "        gt_flatten = {}\n",
    "        for appliance_num, appliance in enumerate(ORDER):\n",
    "            prediction_flatten[appliance] = []\n",
    "            gt_flatten[appliance] = []\n",
    "\n",
    "        for appliance_num, appliance in enumerate(ORDER):\n",
    "            for fold in range(5):\n",
    "                prediction_flatten[appliance].append(preds[fold][appliance_num])\n",
    "                gt_flatten[appliance].append(gts[fold][appliance_num])\n",
    "            gt_flatten[appliance] = np.concatenate(gt_flatten[appliance])\n",
    "            prediction_flatten[appliance] = np.concatenate(prediction_flatten[appliance])\n",
    "\n",
    "        err = {}\n",
    "        for appliance in ORDER:\n",
    "            print(appliance)\n",
    "            err[appliance] = mean_absolute_error(gt_flatten[appliance], prediction_flatten[appliance])\n",
    "        print(err)\n",
    "\n",
    "        np.save(\"./baseline/rnn-aug-pred-{}-{}-{}.npy\".format(dataset, appliance, num_aug), prediction_flatten)\n",
    "        np.save(\"./baseline/rnn-aug-error-{}-{}-{}.npy\".format(dataset, appliance, num_aug), err)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
